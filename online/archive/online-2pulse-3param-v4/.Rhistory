reference_day = 8
for(mouse in 1:num_mice){
parameter_vec= parameter_mat[mouse,]
one.reference = generate_one_counterfactualset(parameter_mat[mouse,],total_days = total_days, wait_time = waitime_vec[1], potential_actions = 1:10)
one.optima= which.min(one.reference[,total_days])+waitime_vec[1]
optimal_actions[mouse]= as.numeric(one.optima)
data_mat[mouse,] = one.reference[1,1:20]
true.mins[mouse]= log(min(one.reference[,total_days]))
for(refday.idx in 1:length(reference_days)){
reference_day = reference_days[refday.idx]
#day15.outcomes[mouse] = one.reference[reference_day-6,total_days]
reference.outcomes[mouse, refday.idx]=log(one.reference[reference_day-6,total_days])
}
one.sequence = data_mat[mouse,]
one.state=sequence_to_state(one.sequence = as.numeric(one.sequence), action.vec = c(), done=T, num_free_pulses = 1)
one.action = get_action(q.fit,one.state, potential_actions = 1:10)+7
selected_actions=c(selected_actions, one.action)
one.agent.treated.sequence = generate_one(one.action+15, parameter_vec=parameter_vec, maxtime=total_days)
agent.outcomes[mouse] = log(one.agent.treated.sequence[total_days])
}
selected_actions
deltamodel = abs(optimal_actions-selected_actions)
delta15 =abs(optimal_actions-reference_days[1])
colors = c("red","blue","orange","purple")
plot(density(delta15-deltamodel), xlab="fixed x loss - agent x loss", main="Density plot", col="red",ylim=c(0,0.5), xlim=c(-10,10))
for(refday.idx in 1:length(reference_days)){
cat("\n info for reference:",reference_days[refday.idx])
deltaref = abs(optimal_actions-reference_days[refday.idx])
lines(density(deltaref - deltamodel), col=colors[refday.idx])
one.delta=deltaref-deltamodel
print(t.test(deltaref-deltamodel))
print(100*(length(test_indices)-length(one.delta[one.delta<0]))/length(test_indices))
}
lines(density(abs(optimal_actions-sample(7:17, num_mice, replace=T)) - deltamodel), col="purple")
legend("topleft", legend=c(paste("day", reference_days)), pch=16, col=colors)
abline(v=0, lty=2)
one.delta=abs(optimal_actions-sample(7:17, num_mice, replace=T)) - deltamodel
mean(one.delta)
length(one.delta[one.delta<0])
source('~/Documents/Dissertation/PAI/online-2pulse-2param/data_generation_fncs.R')
setwd("~/Documents/Dissertation/PAI/online-2pulse-2param")
source("data_generation_fncs.R")
source("env_fncs_2pulse.R")
set.seed(1998)
max_epochs=5000
parameter_mat = make_parameter_mat(max_epochs+100)
total_time=40
total_days=total_time
num_pulses=2
num_free_pulses=num_pulses-1
state_size = 31
bellmann_error = rep(NA, max_epochs)
waitime_vec=rep(7, num_pulses)
state_mat = matrix(NA, nrow = max_epochs, ncol=state_size)
nextstate_mat = matrix(NA, nrow=max_epochs, ncol=state_size)
potential_actions = 1:10
action_size=length(potential_actions)
actions = rep(NA, max_epochs)
dones = rep(NA, max_epochs)
eps=1
eps.vec=c(eps)
eps_decay=1
epsilon_min=.001
minibatch_size=5000
epoch=1
rewards = c()
mouse=0
epoch=1
random.init = as.data.frame(matrix(rnorm(length(state_mat)), nrow=nrow(state_mat), ncol=state_size))
random.init$actions=sample(potential_actions,nrow(state_mat), replace=T)
random.init$actions2 = random.init$actions^2
#q.fit = lm(rnorm(nrow(state_mat))~(.)+(.)*actions+actions:actions, data=random.init)
q.fit = caret::pcaNNet(rnorm(nrow(state_mat))~(.), data=random.init, size=10)
israndom=rep(T, max_epochs)
while(epoch < max_epochs){
mouse=mouse+1
action.vec=c()
current.time=15-2+waitime_vec[1]
parameter_vec=parameter_mat[mouse+101,]
one.sequence = generate_one(radiation_days=action.vec,parameter_vec=parameter_vec,maxtime = current.time)
for(pulse in 1:(num_free_pulses)){
one.state = sequence_to_state(one.sequence, action.vec, done, num_free_pulses=num_free_pulses)
done = (pulse==num_free_pulses)
dones[epoch]=done
state_mat[epoch,]=one.state
if(runif(1)>eps&(epoch>(minibatch_size+1))){one.action = get_action(q.fit, one.state, potential_actions = potential_actions); israndom[epoch]=F}else{one.action=sample(potential_actions,1)}
action.vec=c(action.vec, one.action)
actions[epoch]=one.action
radiation_days = cumsum(action.vec+waitime_vec[1:length(action.vec)])+15
if(done){
current.time = total_time
}else{
current.time = current.time+one.action+waitime_vec[pulse+1]
}
one.next.sequence = generate_one(radiation_days, parameter_vec = parameter_vec, maxtime=current.time)
one.next.state = sequence_to_state(one.sequence,action.vec,done, num_free_pulses=num_free_pulses)
one.reward = -log(one.next.sequence[length(one.next.sequence)])/7
rewards = c(rewards, one.reward)
nextstate_mat[epoch,] = one.next.state
if(epoch>=minibatch_size & (epoch%%minibatch_size==0)){
minibatch_idx = 1:epoch
state_mat_mini = state_mat[minibatch_idx,]
nextstate_mat_mini=nextstate_mat[minibatch_idx,]
actions_mini = actions[minibatch_idx]
rewards_mini = rewards[minibatch_idx]
dones_mini = dones[minibatch_idx]
rewards_mini[dones_mini==0] = rewards_mini[dones_mini==1]
q.fit = replay(q.fit,state_mat_mini,actions_mini,nextstate_mat_mini, rewards_mini, dones_mini)
}
active.eps.decay = eps_decay*(epoch>=minibatch_size) + 1*(epoch<minibatch_size)
eps = eps*active.eps.decay
eps.vec=c(eps.vec, eps)
epoch = epoch+1
cat("\n epoch:", epoch, "reward:",one.reward)
one.sequence=one.next.sequence
}
# if(eps<=epsilon_min){
#   cat("\n finished due to small epsilon")
#   break
# }
}
dones[epoch]=1
inputs=as.data.frame(state_mat)
inputs$actions=actions
inputs$actions2=actions^2
train_idx = setdiff(1:4999,1:100)
q.fit = caret::pcaNNet(rewards[train_idx]~(.), data=inputs[train_idx,], size=30,linout=T, scale=T, maxit=50000)
test_indices=1:100
num_mice = length(test_indices)
optimal_actions=rep(NA, num_mice)
data_mat = matrix(NA, ncol=20, nrow=num_mice)
true.mins = rep(NA, num_mice)
agent.outcomes=rep(NA, num_mice)
reference_days=c(10,14,15)
reference.outcomes=matrix(NA, nrow=num_mice, ncol=length(reference_days))
selected_actions=c()
reference_day = 8
for(mouse in 1:num_mice){
parameter_vec= parameter_mat[mouse,]
one.reference = generate_one_counterfactualset(parameter_mat[mouse,],total_days = total_days, wait_time = waitime_vec[1], potential_actions = 1:10)
one.optima= which.min(one.reference[,total_days])+waitime_vec[1]
optimal_actions[mouse]= as.numeric(one.optima)
data_mat[mouse,] = one.reference[1,1:20]
true.mins[mouse]= log(min(one.reference[,total_days]))
for(refday.idx in 1:length(reference_days)){
reference_day = reference_days[refday.idx]
#day15.outcomes[mouse] = one.reference[reference_day-6,total_days]
reference.outcomes[mouse, refday.idx]=log(one.reference[reference_day-6,total_days])
}
one.sequence = data_mat[mouse,]
one.state=sequence_to_state(one.sequence = as.numeric(one.sequence), action.vec = c(), done=T, num_free_pulses = 1)
one.action = get_action(q.fit,one.state, potential_actions = 1:10)+7
selected_actions=c(selected_actions, one.action)
one.agent.treated.sequence = generate_one(one.action+15, parameter_vec=parameter_vec, maxtime=total_days)
agent.outcomes[mouse] = log(one.agent.treated.sequence[total_days])
}
selected_actions
deltamodel = abs(optimal_actions-selected_actions)
delta15 =abs(optimal_actions-reference_days[1])
colors = c("red","blue","orange","purple")
plot(density(delta15-deltamodel), xlab="fixed x loss - agent x loss", main="Density plot", col="red",ylim=c(0,0.5), xlim=c(-10,10))
for(refday.idx in 1:length(reference_days)){
cat("\n info for reference:",reference_days[refday.idx])
deltaref = abs(optimal_actions-reference_days[refday.idx])
lines(density(deltaref - deltamodel), col=colors[refday.idx])
one.delta=deltaref-deltamodel
print(t.test(deltaref-deltamodel))
print(100*(length(test_indices)-length(one.delta[one.delta<0]))/length(test_indices))
}
lines(density(abs(optimal_actions-sample(7:17, num_mice, replace=T)) - deltamodel), col="purple")
legend("topleft", legend=c(paste("day", reference_days)), pch=16, col=colors)
abline(v=0, lty=2)
table(selected_actions, optimal_actions)
source('~/Documents/Dissertation/PAI/online-2pulse-2param/data_generation_fncs.R')
source('~/Documents/Dissertation/PAI/online-2pulse-2param/online_engine.R')
test_indices=1:100
num_mice = length(test_indices)
optimal_actions=rep(NA, num_mice)
data_mat = matrix(NA, ncol=20, nrow=num_mice)
true.mins = rep(NA, num_mice)
agent.outcomes=rep(NA, num_mice)
reference_days=c(10,14,15)
reference.outcomes=matrix(NA, nrow=num_mice, ncol=length(reference_days))
selected_actions=c()
reference_day = 8
for(mouse in 1:num_mice){
parameter_vec= parameter_mat[mouse,]
one.reference = generate_one_counterfactualset(parameter_mat[mouse,],total_days = total_days, wait_time = waitime_vec[1], potential_actions = 1:10)
one.optima= which.min(one.reference[,total_days])+waitime_vec[1]
optimal_actions[mouse]= as.numeric(one.optima)
data_mat[mouse,] = one.reference[1,1:20]
true.mins[mouse]= log(min(one.reference[,total_days]))
for(refday.idx in 1:length(reference_days)){
reference_day = reference_days[refday.idx]
#day15.outcomes[mouse] = one.reference[reference_day-6,total_days]
reference.outcomes[mouse, refday.idx]=log(one.reference[reference_day-6,total_days])
}
one.sequence = data_mat[mouse,]
one.state=sequence_to_state(one.sequence = as.numeric(one.sequence), action.vec = c(), done=T, num_free_pulses = 1)
one.action = get_action(q.fit,one.state, potential_actions = 1:10)+7
selected_actions=c(selected_actions, one.action)
one.agent.treated.sequence = generate_one(one.action+15, parameter_vec=parameter_vec, maxtime=total_days)
agent.outcomes[mouse] = log(one.agent.treated.sequence[total_days])
}
selected_actions
deltamodel = abs(optimal_actions-selected_actions)
delta15 =abs(optimal_actions-reference_days[1])
colors = c("red","blue","orange","purple")
plot(density(delta15-deltamodel), xlab="fixed x loss - agent x loss", main="Density plot", col="red",ylim=c(0,0.5), xlim=c(-10,10))
for(refday.idx in 1:length(reference_days)){
cat("\n info for reference:",reference_days[refday.idx])
deltaref = abs(optimal_actions-reference_days[refday.idx])
lines(density(deltaref - deltamodel), col=colors[refday.idx])
one.delta=deltaref-deltamodel
print(t.test(deltaref-deltamodel))
print(100*(length(test_indices)-length(one.delta[one.delta<0]))/length(test_indices))
}
lines(density(abs(optimal_actions-sample(7:17, num_mice, replace=T)) - deltamodel), col="purple")
legend("topleft", legend=c(paste("day", reference_days)), pch=16, col=colors)
abline(v=0, lty=2)
deltarandom = abs(optimal_actions-sample(7:17, num_mice, replace=T)
deltarandom = abs(optimal_actions-sample(7:17, num_mice, replace=T))-deltamodel
deltarandom = abs(optimal_actions-sample(7:17, num_mice, replace=T))-deltamodel
t.test(deltarandom)
mean[deltarandom[deltarandom<0]]
mean(deltarandom[deltarandom<0])
mean(deltarandom[deltarandom>0])
table(selected_actions, optima)
table(selected_actions, optimal_actions)
source('~/Documents/Dissertation/PAI/online-2pulse-2param/data_generation_fncs.R')
setwd("~/Documents/Dissertation/PAI/online-2pulse-2param")
source("data_generation_fncs.R")
source("env_fncs_2pulse.R")
set.seed(1998)
max_epochs=5000
parameter_mat = make_parameter_mat(max_epochs+100)
total_time=40
total_days=total_time
num_pulses=2
num_free_pulses=num_pulses-1
state_size = 31
bellmann_error = rep(NA, max_epochs)
waitime_vec=rep(7, num_pulses)
state_mat = matrix(NA, nrow = max_epochs, ncol=state_size)
nextstate_mat = matrix(NA, nrow=max_epochs, ncol=state_size)
potential_actions = 1:10
action_size=length(potential_actions)
actions = rep(NA, max_epochs)
dones = rep(NA, max_epochs)
eps=1
eps.vec=c(eps)
eps_decay=1
epsilon_min=.001
minibatch_size=5000
epoch=1
rewards = c()
mouse=0
epoch=1
random.init = as.data.frame(matrix(rnorm(length(state_mat)), nrow=nrow(state_mat), ncol=state_size))
random.init$actions=sample(potential_actions,nrow(state_mat), replace=T)
random.init$actions2 = random.init$actions^2
#q.fit = lm(rnorm(nrow(state_mat))~(.)+(.)*actions+actions:actions, data=random.init)
q.fit = caret::pcaNNet(rnorm(nrow(state_mat))~(.), data=random.init, size=10)
israndom=rep(T, max_epochs)
while(epoch < max_epochs){
mouse=mouse+1
action.vec=c()
current.time=15-2+waitime_vec[1]
parameter_vec=parameter_mat[mouse+101,]
one.sequence = generate_one(radiation_days=action.vec,parameter_vec=parameter_vec,maxtime = current.time)
for(pulse in 1:(num_free_pulses)){
one.state = sequence_to_state(one.sequence, action.vec, done, num_free_pulses=num_free_pulses)
done = (pulse==num_free_pulses)
dones[epoch]=done
state_mat[epoch,]=one.state
if(runif(1)>eps&(epoch>(minibatch_size+1))){one.action = get_action(q.fit, one.state, potential_actions = potential_actions); israndom[epoch]=F}else{one.action=sample(potential_actions,1)}
action.vec=c(action.vec, one.action)
actions[epoch]=one.action
radiation_days = cumsum(action.vec+waitime_vec[1:length(action.vec)])+15
if(done){
current.time = total_time
}else{
current.time = current.time+one.action+waitime_vec[pulse+1]
}
one.next.sequence = generate_one(radiation_days, parameter_vec = parameter_vec, maxtime=current.time)
one.next.state = sequence_to_state(one.sequence,action.vec,done, num_free_pulses=num_free_pulses)
one.reward = -log(one.next.sequence[length(one.next.sequence)])/7
rewards = c(rewards, one.reward)
nextstate_mat[epoch,] = one.next.state
if(epoch>=minibatch_size & (epoch%%minibatch_size==0)){
minibatch_idx = 1:epoch
state_mat_mini = state_mat[minibatch_idx,]
nextstate_mat_mini=nextstate_mat[minibatch_idx,]
actions_mini = actions[minibatch_idx]
rewards_mini = rewards[minibatch_idx]
dones_mini = dones[minibatch_idx]
rewards_mini[dones_mini==0] = rewards_mini[dones_mini==1]
q.fit = replay(q.fit,state_mat_mini,actions_mini,nextstate_mat_mini, rewards_mini, dones_mini)
}
active.eps.decay = eps_decay*(epoch>=minibatch_size) + 1*(epoch<minibatch_size)
eps = eps*active.eps.decay
eps.vec=c(eps.vec, eps)
epoch = epoch+1
cat("\n epoch:", epoch, "reward:",one.reward)
one.sequence=one.next.sequence
}
# if(eps<=epsilon_min){
#   cat("\n finished due to small epsilon")
#   break
# }
}
dones[epoch]=1
inputs=as.data.frame(state_mat)
inputs$actions=actions
inputs$actions2=actions^2
train_idx = setdiff(1:4999,1:100)
q.fit = caret::pcaNNet(rewards[train_idx]~(.), data=inputs[train_idx,], size=30,linout=T, scale=T, maxit=50000)
test_indices=1:100
num_mice = length(test_indices)
optimal_actions=rep(NA, num_mice)
data_mat = matrix(NA, ncol=20, nrow=num_mice)
true.mins = rep(NA, num_mice)
agent.outcomes=rep(NA, num_mice)
reference_days=c(8,10,15,17)
reference.outcomes=matrix(NA, nrow=num_mice, ncol=length(reference_days))
selected_actions=c()
reference_day = 8
for(mouse in 1:num_mice){
parameter_vec= parameter_mat[mouse,]
one.reference = generate_one_counterfactualset(parameter_mat[mouse,],total_days = total_days, wait_time = waitime_vec[1], potential_actions = 1:10)
one.optima= which.min(one.reference[,total_days])+waitime_vec[1]
optimal_actions[mouse]= as.numeric(one.optima)
data_mat[mouse,] = one.reference[1,1:20]
true.mins[mouse]= log(min(one.reference[,total_days]))
for(refday.idx in 1:length(reference_days)){
reference_day = reference_days[refday.idx]
#day15.outcomes[mouse] = one.reference[reference_day-6,total_days]
reference.outcomes[mouse, refday.idx]=log(one.reference[reference_day-6,total_days])
}
one.sequence = data_mat[mouse,]
one.state=sequence_to_state(one.sequence = as.numeric(one.sequence), action.vec = c(), done=T, num_free_pulses = 1)
one.action = get_action(q.fit,one.state, potential_actions = 1:10)+7
selected_actions=c(selected_actions, one.action)
one.agent.treated.sequence = generate_one(one.action+15, parameter_vec=parameter_vec, maxtime=total_days)
agent.outcomes[mouse] = log(one.agent.treated.sequence[total_days])
}
selected_actions
deltamodel = abs(optimal_actions-selected_actions)
delta15 =abs(optimal_actions-reference_days[1])
colors = c("red","blue","orange","purple")
plot(density(delta15-deltamodel), xlab="fixed x loss - agent x loss", main="Density plot", col="red",ylim=c(0,0.5), xlim=c(-10,10))
for(refday.idx in 1:length(reference_days)){
cat("\n info for reference:",reference_days[refday.idx])
deltaref = abs(optimal_actions-reference_days[refday.idx])
lines(density(deltaref - deltamodel), col=colors[refday.idx])
one.delta=deltaref-deltamodel
print(t.test(deltaref-deltamodel))
print(100*(length(test_indices)-length(one.delta[one.delta<0]))/length(test_indices))
}
deltarandom = abs(optimal_actions-sample(7:17, num_mice, replace=T))-deltamodel
lines(density(abs(optimal_actions-sample(7:17, num_mice, replace=T)) - deltamodel), col="purple")
legend("topleft", legend=c(paste("day", reference_days)), pch=16, col=colors)
abline(v=0, lty=2)
test_indices=1:100
num_mice = length(test_indices)
optimal_actions=rep(NA, num_mice)
data_mat = matrix(NA, ncol=20, nrow=num_mice)
true.mins = rep(NA, num_mice)
agent.outcomes=rep(NA, num_mice)
reference_days=c(8,10,15)
reference.outcomes=matrix(NA, nrow=num_mice, ncol=length(reference_days))
selected_actions=c()
reference_day = 8
for(mouse in 1:num_mice){
parameter_vec= parameter_mat[mouse,]
one.reference = generate_one_counterfactualset(parameter_mat[mouse,],total_days = total_days, wait_time = waitime_vec[1], potential_actions = 1:10)
one.optima= which.min(one.reference[,total_days])+waitime_vec[1]
optimal_actions[mouse]= as.numeric(one.optima)
data_mat[mouse,] = one.reference[1,1:20]
true.mins[mouse]= log(min(one.reference[,total_days]))
for(refday.idx in 1:length(reference_days)){
reference_day = reference_days[refday.idx]
#day15.outcomes[mouse] = one.reference[reference_day-6,total_days]
reference.outcomes[mouse, refday.idx]=log(one.reference[reference_day-6,total_days])
}
one.sequence = data_mat[mouse,]
one.state=sequence_to_state(one.sequence = as.numeric(one.sequence), action.vec = c(), done=T, num_free_pulses = 1)
one.action = get_action(q.fit,one.state, potential_actions = 1:10)+7
selected_actions=c(selected_actions, one.action)
one.agent.treated.sequence = generate_one(one.action+15, parameter_vec=parameter_vec, maxtime=total_days)
agent.outcomes[mouse] = log(one.agent.treated.sequence[total_days])
}
selected_actions
deltamodel = abs(optimal_actions-selected_actions)
delta15 =abs(optimal_actions-reference_days[1])
colors = c("red","blue","orange","purple")
plot(density(delta15-deltamodel), xlab="fixed x loss - agent x loss", main="Density plot", col="red",ylim=c(0,0.5), xlim=c(-10,10))
for(refday.idx in 1:length(reference_days)){
cat("\n info for reference:",reference_days[refday.idx])
deltaref = abs(optimal_actions-reference_days[refday.idx])
lines(density(deltaref - deltamodel), col=colors[refday.idx])
one.delta=deltaref-deltamodel
print(t.test(deltaref-deltamodel))
print(100*(length(test_indices)-length(one.delta[one.delta<0]))/length(test_indices))
}
deltarandom = abs(optimal_actions-sample(7:17, num_mice, replace=T))-deltamodel
lines(density(abs(optimal_actions-sample(7:17, num_mice, replace=T)) - deltamodel), col="purple")
legend("topleft", legend=c(paste("day", reference_days)), pch=16, col=colors)
abline(v=0, lty=2)
table(selected_actions, optimal_actions)
q.fit = caret::pcaNNet(rewards[train_idx]~(.)^2, data=inputs[train_idx,], size=30,linout=T, scale=T, maxit=50000)
test_indices=1:100
num_mice = length(test_indices)
optimal_actions=rep(NA, num_mice)
data_mat = matrix(NA, ncol=20, nrow=num_mice)
true.mins = rep(NA, num_mice)
agent.outcomes=rep(NA, num_mice)
reference_days=c(8,10,15)
reference.outcomes=matrix(NA, nrow=num_mice, ncol=length(reference_days))
selected_actions=c()
reference_day = 8
for(mouse in 1:num_mice){
parameter_vec= parameter_mat[mouse,]
one.reference = generate_one_counterfactualset(parameter_mat[mouse,],total_days = total_days, wait_time = waitime_vec[1], potential_actions = 1:10)
one.optima= which.min(one.reference[,total_days])+waitime_vec[1]
optimal_actions[mouse]= as.numeric(one.optima)
data_mat[mouse,] = one.reference[1,1:20]
true.mins[mouse]= log(min(one.reference[,total_days]))
for(refday.idx in 1:length(reference_days)){
reference_day = reference_days[refday.idx]
#day15.outcomes[mouse] = one.reference[reference_day-6,total_days]
reference.outcomes[mouse, refday.idx]=log(one.reference[reference_day-6,total_days])
}
one.sequence = data_mat[mouse,]
one.state=sequence_to_state(one.sequence = as.numeric(one.sequence), action.vec = c(), done=T, num_free_pulses = 1)
one.action = get_action(q.fit,one.state, potential_actions = 1:10)+7
selected_actions=c(selected_actions, one.action)
one.agent.treated.sequence = generate_one(one.action+15, parameter_vec=parameter_vec, maxtime=total_days)
agent.outcomes[mouse] = log(one.agent.treated.sequence[total_days])
}
selected_actions
deltamodel = abs(optimal_actions-selected_actions)
delta15 =abs(optimal_actions-reference_days[1])
colors = c("red","blue","orange","purple")
plot(density(delta15-deltamodel), xlab="fixed x loss - agent x loss", main="Density plot", col="red",ylim=c(0,0.5), xlim=c(-10,10))
for(refday.idx in 1:length(reference_days)){
cat("\n info for reference:",reference_days[refday.idx])
deltaref = abs(optimal_actions-reference_days[refday.idx])
lines(density(deltaref - deltamodel), col=colors[refday.idx])
one.delta=deltaref-deltamodel
print(t.test(deltaref-deltamodel))
print(100*(length(test_indices)-length(one.delta[one.delta<0]))/length(test_indices))
}
deltarandom = abs(optimal_actions-sample(7:17, num_mice, replace=T))-deltamodel
lines(density(abs(optimal_actions-sample(7:17, num_mice, replace=T)) - deltamodel), col="purple")
legend("topleft", legend=c(paste("day", reference_days)), pch=16, col=colors)
abline(v=0, lty=2)
table(optimal_actions, selected_actions)
reference_days=c(8,10,14)
reference.outcomes=matrix(NA, nrow=num_mice, ncol=length(reference_days))
selected_actions=c()
reference_day = 8
for(mouse in 1:num_mice){
parameter_vec= parameter_mat[mouse,]
one.reference = generate_one_counterfactualset(parameter_mat[mouse,],total_days = total_days, wait_time = waitime_vec[1], potential_actions = 1:10)
one.optima= which.min(one.reference[,total_days])+waitime_vec[1]
optimal_actions[mouse]= as.numeric(one.optima)
data_mat[mouse,] = one.reference[1,1:20]
true.mins[mouse]= log(min(one.reference[,total_days]))
for(refday.idx in 1:length(reference_days)){
reference_day = reference_days[refday.idx]
#day15.outcomes[mouse] = one.reference[reference_day-6,total_days]
reference.outcomes[mouse, refday.idx]=log(one.reference[reference_day-6,total_days])
}
one.sequence = data_mat[mouse,]
one.state=sequence_to_state(one.sequence = as.numeric(one.sequence), action.vec = c(), done=T, num_free_pulses = 1)
one.action = get_action(q.fit,one.state, potential_actions = 1:10)+7
selected_actions=c(selected_actions, one.action)
one.agent.treated.sequence = generate_one(one.action+15, parameter_vec=parameter_vec, maxtime=total_days)
agent.outcomes[mouse] = log(one.agent.treated.sequence[total_days])
}
selected_actions
deltamodel = abs(optimal_actions-selected_actions)
delta15 =abs(optimal_actions-reference_days[1])
colors = c("red","blue","orange","purple")
plot(density(delta15-deltamodel), xlab="fixed x loss - agent x loss", main="Density plot", col="red",ylim=c(0,0.5), xlim=c(-10,10))
for(refday.idx in 1:length(reference_days)){
cat("\n info for reference:",reference_days[refday.idx])
deltaref = abs(optimal_actions-reference_days[refday.idx])
lines(density(deltaref - deltamodel), col=colors[refday.idx])
one.delta=deltaref-deltamodel
print(t.test(deltaref-deltamodel))
print(100*(length(test_indices)-length(one.delta[one.delta<0]))/length(test_indices))
}
deltarandom = abs(optimal_actions-sample(7:17, num_mice, replace=T))-deltamodel
lines(density(abs(optimal_actions-sample(7:17, num_mice, replace=T)) - deltamodel), col="purple")
legend("topleft", legend=c(paste("day", reference_days)), pch=16, col=colors)
abline(v=0, lty=2)
deltamodel = agent.outcomes-true.mins
delta15 =reference.outcomes[,1]-true.mins
colors = c("red","blue","orange","purple")
plot(density(delta15-deltamodel), xlab="fixed y loss - agent y loss", main="Density plot", col="red", ylim=c(0,60))
for(refday.idx in 2:length(reference_days)){
deltaref = reference.outcomes[,refday.idx]-true.mins
lines(density(deltaref - deltamodel), col=colors[refday.idx])
cat(deltamodel[deltamodel>deltaref])
}
lines(density(abs(optimal_actions-sample(7:17, num_mice, replace=T)) - deltamodel), col="purple")
legend("topleft", legend=c(paste("day", reference_days), "random"), pch=16, col=c(colors, "purple"))
abline(v=0, lty=2)
table(optimal_actions)
q.fit = caret::pcaNNet(rewards[train_idx]~(.)^3, data=inputs[train_idx,], size=30,linout=T, scale=T, maxit=50000)
