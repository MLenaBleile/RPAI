current.time = total_time
}else{
current.time = current.time+one.action+waitime_vec[pulse+1]
}
one.next.sequence = generate_one(radiation_days, parameter_vec = parameter_vec, maxtime=current.time)
one.next.state = sequence_to_state(one.sequence,action.vec,done, num_free_pulses=num_free_pulses)
one.reward = -log(one.next.sequence[length(one.next.sequence)])/7
rewards = c(rewards, one.reward)
nextstate_mat[epoch,] = one.next.state
if(epoch>=minibatch_size){
minibatch_idx = sample(1:epoch, minibatch_size)
state_mat_mini = state_mat[minibatch_idx,]
nextstate_mat_mini=nextstate_mat[minibatch_idx,]
actions_mini = actions[minibatch_idx]
rewards_mini = rewards[minibatch_idx]
dones_mini = dones[minibatch_idx]
rewards_mini[dones_mini==0] = rewards_mini[dones_mini==1]
q.fit = replay(q.fit,state_mat_mini,actions_mini,nextstate_mat_mini, rewards_mini, dones_mini)
active.eps.decay = eps_decay*(epoch>=minibatch_size) + 1*(epoch<minibatch_size)
eps = eps*active.eps.decay
}
eps.vec=c(eps.vec, eps)
epoch = epoch+1
cat("\n epoch:", epoch, "reward:",one.reward)
one.sequence=one.next.sequence
}
# if(eps<=epsilon_min){
#   cat("\n finished due to small epsilon")
#   break
# }
}
q.fit = neuralnet(formula = V1~(.)^2 , data=random.init, hidden = c(10,6), threshold = 10000,stepmax = 2, rep = 2)
israndom=rep(T, max_epochs)
while(epoch < max_epochs){
mouse=mouse+1
action.vec=c()
current.time=15-2+waitime_vec[1]
parameter_vec=parameter_mat[mouse+501,]
one.sequence = generate_one(radiation_days=action.vec,parameter_vec=parameter_vec,maxtime = current.time)
for(pulse in 1:(num_free_pulses)){
one.state = sequence_to_state(one.sequence, action.vec, done, num_free_pulses=num_free_pulses)
done = (pulse==num_free_pulses)
dones[epoch]=done
state_mat[epoch,]=one.state
if(runif(1)>eps&(epoch>(minibatch_size+1))){one.action = get_action(q.fit, one.state, potential_actions = potential_actions); israndom[epoch]=F}else{one.action=sample(potential_actions,1)}
action.vec=c(action.vec, one.action)
actions[epoch]=one.action
radiation_days = cumsum(action.vec+waitime_vec[1:length(action.vec)])+15
if(done){
current.time = total_time
}else{
current.time = current.time+one.action+waitime_vec[pulse+1]
}
one.next.sequence = generate_one(radiation_days, parameter_vec = parameter_vec, maxtime=current.time)
one.next.state = sequence_to_state(one.sequence,action.vec,done, num_free_pulses=num_free_pulses)
one.reward = -log(one.next.sequence[length(one.next.sequence)])/7
rewards = c(rewards, one.reward)
nextstate_mat[epoch,] = one.next.state
if(epoch>=minibatch_size){
minibatch_idx = sample(1:epoch, minibatch_size)
state_mat_mini = state_mat[minibatch_idx,]
nextstate_mat_mini=nextstate_mat[minibatch_idx,]
actions_mini = actions[minibatch_idx]
rewards_mini = rewards[minibatch_idx]
dones_mini = dones[minibatch_idx]
rewards_mini[dones_mini==0] = rewards_mini[dones_mini==1]
q.fit = replay(q.fit,state_mat_mini,actions_mini,nextstate_mat_mini, rewards_mini, dones_mini)
active.eps.decay = eps_decay*(epoch>=minibatch_size) + 1*(epoch<minibatch_size)
eps = eps*active.eps.decay
}
eps.vec=c(eps.vec, eps)
epoch = epoch+1
cat("\n epoch:", epoch, "reward:",one.reward)
one.sequence=one.next.sequence
}
# if(eps<=epsilon_min){
#   cat("\n finished due to small epsilon")
#   break
# }
}
q.fit = neuralnet(formula = V1~(.)*(.) , data=random.init, hidden = c(10,6), threshold = 10000,stepmax = 2, rep = 2)
q.fit = neuralnet(formula = V1~(.)*actions , data=random.init, hidden = c(10,6), threshold = 10000,stepmax = 2, rep = 2)
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
setwd("~/Documents/Dissertation/RPAI/online/online-2pulse-working")
source("data_generation_fncs.R")
source("env_fncs_2pulse.R")
set.seed(1998)
max_epochs=4500
parameter_mat = make_parameter_mat(max_epochs+500)
total_time=40
total_days=total_time
num_pulses=2
num_free_pulses=num_pulses-1
state_size = 40
bellmann_error = rep(NA, max_epochs)
waitime_vec=rep(7, num_pulses)
state_mat = matrix(NA, nrow = max_epochs, ncol=state_size)
nextstate_mat = matrix(NA, nrow=max_epochs, ncol=state_size)
potential_actions = 1:13
action_size=length(potential_actions)
actions = rep(NA, max_epochs)
dones = rep(NA, max_epochs)
eps=1
eps.vec=c(eps)
eps_decay=.9999
epsilon_min=.001
minibatch_size=64
epoch=1
rewards = c()
mouse=0
epoch=1
random.init = as.data.frame(matrix(rnorm(length(state_mat)), nrow=nrow(state_mat), ncol=state_size))
random.init$actions=sample(potential_actions,nrow(state_mat), replace=T)
random.init$actions2 = random.init$actions^2
library(neuralnet)
q.fit = neuralnet(formula = V1~(.) , data=random.init, hidden = c(10,6), threshold = 10000,stepmax = 2, rep = 2)
israndom=rep(T, max_epochs)
q.fit = neuralnet(formula = V1~(.) , data=random.init, hidden = c(10,6), threshold = 10000,stepmax = 2, rep = 2)
head(random.init)
q.fit = neuralnet(formula = V1~(.) , data=random.init, hidden = c(10,6), threshold = 10000,stepmax = 2, rep = 2)
israndom=rep(T, max_epochs)
while(epoch < max_epochs){
mouse=mouse+1
action.vec=c()
current.time=15-2+waitime_vec[1]
parameter_vec=parameter_mat[mouse+501,]
one.sequence = generate_one(radiation_days=action.vec,parameter_vec=parameter_vec,maxtime = current.time)
for(pulse in 1:(num_free_pulses)){
one.state = sequence_to_state(one.sequence, action.vec, done, num_free_pulses=num_free_pulses)
done = (pulse==num_free_pulses)
dones[epoch]=done
state_mat[epoch,]=one.state
if(runif(1)>eps&(epoch>(minibatch_size+1))){one.action = get_action(q.fit, one.state, potential_actions = potential_actions); israndom[epoch]=F}else{one.action=sample(potential_actions,1)}
action.vec=c(action.vec, one.action)
actions[epoch]=one.action
radiation_days = cumsum(action.vec+waitime_vec[1:length(action.vec)])+15
if(done){
current.time = total_time
}else{
current.time = current.time+one.action+waitime_vec[pulse+1]
}
one.next.sequence = generate_one(radiation_days, parameter_vec = parameter_vec, maxtime=current.time)
one.next.state = sequence_to_state(one.sequence,action.vec,done, num_free_pulses=num_free_pulses)
one.reward = -log(one.next.sequence[length(one.next.sequence)])/7
rewards = c(rewards, one.reward)
nextstate_mat[epoch,] = one.next.state
if(epoch>=minibatch_size){
minibatch_idx = sample(1:epoch, minibatch_size)
state_mat_mini = state_mat[minibatch_idx,]
nextstate_mat_mini=nextstate_mat[minibatch_idx,]
actions_mini = actions[minibatch_idx]
rewards_mini = rewards[minibatch_idx]
dones_mini = dones[minibatch_idx]
rewards_mini[dones_mini==0] = rewards_mini[dones_mini==1]
q.fit = replay(q.fit,state_mat_mini,actions_mini,nextstate_mat_mini, rewards_mini, dones_mini)
active.eps.decay = eps_decay*(epoch>=minibatch_size) + 1*(epoch<minibatch_size)
eps = eps*active.eps.decay
}
eps.vec=c(eps.vec, eps)
epoch = epoch+1
cat("\n epoch:", epoch, "reward:",one.reward)
one.sequence=one.next.sequence
}
# if(eps<=epsilon_min){
#   cat("\n finished due to small epsilon")
#   break
# }
}
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
select.list()
selected_actions
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
table(selected_actions, optimal_actions)
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
table(selected_actions, optimal_actions)
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
table(selected_actions, optimal_actions)
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
table(selected_actions, optimal_actions)
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
eps
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
table(selected_actions, optimal_actions)
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
saveRDS(q.fit, "model.rds")
setwd("~/Documents/Dissertation/RPAI/online/online-2pulse-3param-v5")
saveRDS(q.fit, "model.rds")
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
q.fit = nnet::nnet(rewards[train_idx]~(.)^2, data=inputs[train_idx,], size=10,linout=T, scale=T, maxit=50000)
q.fit = nnet::nnet(rewards[train_idx]~(.), data=inputs[train_idx,], size=30,linout=T, scale=T, maxit=50000)
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
getwd()
saveRDS(q.fit,"model.rds")
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
minisize=nrow(state_mat_mini)
targets=rep(NA, minisize)
for(idx in 1:minisize){
target = rewards_mini[idx]
targets[idx] = target
}
inputs=as.data.frame(state_mat_mini)
inputs$actions=actions_mini
inputs$actions2=actions_mini^2
inputs$targets=targets
inputs
ncol(inputs)
ncol(random.init)
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
q.fit = nnet::nnet(rewards[train_idx]~(.), data=inputs[train_idx,],size=30,linout=T, scale=T, maxit=50000)
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
table(selected_actions, optimal_actions)
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
table(optimal_actions)
table(selected_actions, optimal_actions)
saveRDS(q.fit,"model.rds")
par(mfrow=c(2,2))
setwd("~/Documents/Dissertation/RPAI/online/online-2pulse-3param-v5")
source("test_engine.R")
setwd("~/Documents/Dissertation/RPAI/online/online-3pulse-3param-v4")
source("test_engine_multi.R")
setwd("~/Documents/Dissertation/RPAI/online/online-4pulse-3param")
source("test_engine_multi.R")
setwd("~/Documents/Dissertation/RPAI/online/online-5pulse-3param")
source("test_engine_multi.R")
par(mfrow=c(2,2))
setwd("~/Documents/Dissertation/RPAI/online/online-2pulse-3param-v5")
source("test_engine.R")
setwd("~/Documents/Dissertation/RPAI/online/online-3pulse-3param-v4")
source("test_engine_multi.R")
setwd("~/Documents/Dissertation/RPAI/online/online-4pulse-3param")
source("test_engine_multi.R")
setwd("~/Documents/Dissertation/RPAI/online/online-5pulse-3param")
source("test_engine_multi.R")
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-2pulse-working/test_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-3pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-3pulse-working/online_engine.R')
one.sequence
ncol(one.state)
ncol(state_mat)
length(one.state)
length(one.sequence)
source('~/Documents/Dissertation/RPAI/online/online-3pulse-working/online_engine.R')
length(one.state)
source('~/Documents/Dissertation/RPAI/online/online-3pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-3pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-3pulse-working/test_engine_multi.R')
agent.action.mat
source('~/Documents/Dissertation/RPAI/online/online-4pulse-working/online_engine.R')
length(one.state)
source('~/Documents/Dissertation/RPAI/online/online-4pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-4pulse-working/online_engine.R')
test_indices=1:test_num
num_mice = length(test_indices)
optimal_actions=rep(NA, num_mice)
data_mat = matrix(NA, ncol=20, nrow=num_mice)
true.mins = rep(NA, num_mice)
agent.outcomes=rep(NA, num_mice)
day15.outcomes=rep(NA, num_mice)
selected_actions=c()
agent.action.mat = matrix(NA, nrow=num_mice, ncol=num_free_pulses)
reference_days=c(8)
agent.outcome.vec=rep(NA, num_mice)
refdays = c(10,14,25)
ref.outcome.mat = matrix(NA, nrow=num_mice, ncol=length(refdays)+1)
effect.sizes = rep(NA, ncol(ref.outcome.mat))
references = c(paste("day", refdays), "random")
names(effect.sizes) = references
random.action.mat = agent.action.mat
colnames(ref.outcome.mat) = references
for(mouse in 1:num_mice){
parameter_vec=parameter_mat[mouse,]
action.vec=c()
starting.time=15-2+waitime_vec[1]
first.action = 15
one.sequence = generate_one(action.vec, parameter_vec, maxtime=starting.time)
while(length(action.vec)<num_free_pulses){
isdone=(length(action.vec)==(num_free_pulses-1))
one.state=sequence_to_state(one.sequence = as.numeric(one.sequence), action.vec = action.vec, done=isdone, total_time=total_time,num_free_pulses = num_free_pulses)
one.action = get_action(q.fit,one.state, potential_actions = potential_actions)+waitime_vec[1]
action.vec=c(action.vec, one.action)
rt.days = cumsum(action.vec)+first.action
seqlength = (max(rt.days)+waitime_vec[1]-2)*(1-isdone)+total_time*isdone
one.sequence = generate_one(rt.days, parameter_vec, maxtime = seqlength)
current.time = cumsum(action.vec)+first.action
}
for(ref.idx in 1:length(refdays)){
ref = refdays[ref.idx]
one.reference = generate_one(cumsum(c(ref,ref, ref))+first.action, parameter_vec, maxtime = seqlength)
ref.outcome.mat[mouse,ref.idx]=log(one.reference[seqlength])
}
one.random = sample(potential_actions,num_free_pulses, replace=T)
random.action.mat[mouse,] = one.random
one.random = one.random+waitime_vec
ref.outcome.mat[mouse,length(refdays)+1]= log(generate_one(cumsum(one.random)+15, parameter_vec, maxtime = seqlength))[seqlength]
agent.action.mat[mouse,]=action.vec
agent.outcome.vec[mouse] = log(one.sequence[seqlength])
}
adj.val=1
plot(density(-agent.outcome.vec+ref.outcome.mat[,1],adjust=adj.val), col="red",type="n", ylim=c(0,5),xlab="final ltv reference-final ltv agent", main="4 pulse performance", xlim=c(-.4,.7))
colors= c("red","blue", "orange", "purple")
for(ref.idx in 1:(ncol(ref.outcome.mat))){
ref = refdays[ref.idx]
lines(density(ref.outcome.mat[,ref.idx] - agent.outcome.vec, adjust=adj.val+4*(ref.idx==4)), col=colors[ref.idx])
print(t.test(ref.outcome.mat[,ref.idx] - agent.outcome.vec))
delta = ref.outcome.mat[,ref.idx] - agent.outcome.vec
delta2 = ref.outcome.mat[,ref.idx] - ref.outcome.mat[,4]
effect.sizes[[references[ref.idx]]] = mean(delta)/sd(delta)
print(summary(delta))
print(length(delta[delta<0])/100)
print(sum(delta2<0)/100)
}
legend("topleft", legend=paste(c(paste("day", refdays),"random"), round(effect.sizes, digits=3), sep=": d="), pch=16, col=colors)
abline(v=0, lty=2)
mean(-agent.outcome.vec+ref.outcome.mat[,1])
source('~/Documents/Dissertation/RPAI/online/online-4pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-4pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-4pulse-working/test_engine_multi.R')
source('~/Documents/Dissertation/RPAI/online/online-4pulse-3param/test_engine_multi.R')
source('~/Documents/Dissertation/RPAI/online/online-4pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-4pulse-working/test_engine_multi.R')
source('~/Documents/Dissertation/RPAI/online/online-4pulse-working/test_engine_multi.R')
source('~/Documents/Dissertation/RPAI/online/online-4pulse-3param/test_engine_multi.R')
source('~/Documents/Dissertation/RPAI/online/online-4pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-4pulse-working/test_engine_multi.R')
source('~/Documents/Dissertation/RPAI/online/online-4pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-4pulse-working/test_engine_multi.R')
source('~/Documents/Dissertation/RPAI/online/online-4pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-4pulse-working/test_engine_multi.R')
source('~/Documents/Dissertation/RPAI/online/online-4pulse-working/online_engine.R')
minibatch_idx = 1:max_epochs
q.fit = replay(q.fit,state_mat[minibatch_idx,],actions[minibatch_idx],nextstate_mat[minibatch_idx,], rewards[minibatch_idx], dones[minibatch_idx])
source('~/Documents/Dissertation/RPAI/online/online-5pulse-working/online_engine.R')
length(one.state)
source('~/Documents/Dissertation/RPAI/online/online-5pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-5pulse-working/test_engine_multi.R')
source('~/Documents/Dissertation/RPAI/online/online-5pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-5pulse-working/test_engine_multi.R')
source('~/Documents/Dissertation/RPAI/online/online-5pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-5pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-5pulse-working/test_engine_multi.R')
source('~/Documents/Dissertation/RPAI/online/online-5pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-5pulse-working/test_engine_multi.R')
num_free_pulses
agent.action.mat
source('~/Documents/Dissertation/RPAI/online/online-5pulse-working/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-5pulse-working/test_engine_multi.R')
source('~/Documents/Dissertation/RPAI/online/online-5pulse-3param/online_engine.R')
source('~/Documents/Dissertation/RPAI/online/online-5pulse-working/test_engine_multi.R')
test_indices=1:test_num
num_mice = length(test_indices)
optimal_actions=rep(NA, num_mice)
data_mat = matrix(NA, ncol=20, nrow=num_mice)
true.mins = rep(NA, num_mice)
agent.outcomes=rep(NA, num_mice)
day15.outcomes=rep(NA, num_mice)
selected_actions=c()
agent.action.mat = matrix(NA, nrow=num_mice, ncol=num_free_pulses)
reference_days=c(8)
agent.outcome.vec=rep(NA, test_num)
refdays = c(11,14,20)
ref.outcome.mat = matrix(NA, nrow=num_mice, ncol=length(refdays)+1)
references = c(paste("day", refdays),"random")
colnames(ref.outcome.mat) = references
effect.sizes = rep(NA, ncol(ref.outcome.mat))
names(effect.sizes) = references
random.sds = c()
for(mouse in 1:test_num){
parameter_vec=parameter_mat[mouse,]
action.vec=c()
starting.time=15-2+waitime_vec[1]
first.action = 15
one.sequence = generate_one(action.vec, parameter_vec, maxtime=starting.time)
while(length(action.vec)<num_free_pulses){
isdone=(length(action.vec)==(num_free_pulses-1))
one.state=sequence_to_state(one.sequence = as.numeric(one.sequence), action.vec = action.vec, done=isdone, total_time=total_time,num_free_pulses = num_free_pulses)
one.action = get_action(q.fit,one.state, potential_actions = potential_actions)+waitime_vec[1]
action.vec=c(action.vec, one.action)
rt.days = cumsum(action.vec)+15
seqlength = (max(rt.days)+waitime_vec[1]-2)*(1-isdone)+total_time*isdone
one.sequence = generate_one(rt.days, parameter_vec, maxtime = seqlength)
current.time = cumsum(action.vec)+15
}
for(ref.idx in 1:length(refdays)){
ref = refdays[ref.idx]
one.reference = generate_one(cumsum(rep(ref,num_free_pulses))+15, parameter_vec, maxtime = seqlength)
ref.outcome.mat[mouse,ref.idx]=log(one.reference[seqlength])
}
one.random = sample(potential_actions,num_free_pulses, replace=T)
random.sds = c(random.sds, sd(one.random))
one.random = one.random+waitime_vec
ref.outcome.mat[mouse,length(refdays)+1]= log(generate_one(cumsum(one.random)+15, parameter_vec, maxtime = seqlength))[seqlength]
agent.action.mat[mouse,]=action.vec
agent.outcome.vec[mouse] = log(one.sequence[seqlength])
}
adj.val=1
plot(density(-agent.outcome.vec+ref.outcome.mat[,'random'],adjust=adj.val),col="purple",xlab="final ltv reference-final ltv agent", main="5 pulse performance")
colors= c("red","blue", "orange", "purple")
for(ref.idx in 1:(ncol(ref.outcome.mat))){
ref = refdays[ref.idx]
lines(density(ref.outcome.mat[,ref.idx] - agent.outcome.vec, adjust=adj.val), col=colors[ref.idx])
print(t.test(ref.outcome.mat[,ref.idx] - agent.outcome.vec))
delta = ref.outcome.mat[,ref.idx] - agent.outcome.vec
delta2 = ref.outcome.mat[,ref.idx] - ref.outcome.mat[,4]
effect.sizes[[references[ref.idx]]] = mean(delta)/sd(delta)
print(summary(delta))
print(length(delta[delta<=0])/test_num)
print(sum(delta2<0)/100)
}
legend("topright", legend=paste(c(paste("day", refdays),"random"), round(effect.sizes, digits=3), sep=": d="), pch=16, col=colors)
abline(v=0, lty=2)
mean(-agent.outcome.vec+ref.outcome.mat[,1])
setwd("~/Documents/Dissertation/RPAI/online/online-5pulse-3param")
q.fit=readRDS("model.rds")
test_indices=1:test_num
num_mice = length(test_indices)
optimal_actions=rep(NA, num_mice)
data_mat = matrix(NA, ncol=20, nrow=num_mice)
true.mins = rep(NA, num_mice)
agent.outcomes=rep(NA, num_mice)
day15.outcomes=rep(NA, num_mice)
selected_actions=c()
agent.action.mat = matrix(NA, nrow=num_mice, ncol=num_free_pulses)
reference_days=c(8)
agent.outcome.vec=rep(NA, test_num)
refdays = c(11,14,20)
ref.outcome.mat = matrix(NA, nrow=num_mice, ncol=length(refdays)+1)
references = c(paste("day", refdays),"random")
colnames(ref.outcome.mat) = references
effect.sizes = rep(NA, ncol(ref.outcome.mat))
names(effect.sizes) = references
random.sds = c()
for(mouse in 1:test_num){
parameter_vec=parameter_mat[mouse,]
action.vec=c()
starting.time=15-2+waitime_vec[1]
first.action = 15
one.sequence = generate_one(action.vec, parameter_vec, maxtime=starting.time)
while(length(action.vec)<num_free_pulses){
isdone=(length(action.vec)==(num_free_pulses-1))
one.state=sequence_to_state(one.sequence = as.numeric(one.sequence), action.vec = action.vec, done=isdone, total_time=total_time,num_free_pulses = num_free_pulses)
one.action = get_action(q.fit,one.state, potential_actions = potential_actions)+waitime_vec[1]
action.vec=c(action.vec, one.action)
rt.days = cumsum(action.vec)+15
seqlength = (max(rt.days)+waitime_vec[1]-2)*(1-isdone)+total_time*isdone
one.sequence = generate_one(rt.days, parameter_vec, maxtime = seqlength)
current.time = cumsum(action.vec)+15
}
for(ref.idx in 1:length(refdays)){
ref = refdays[ref.idx]
one.reference = generate_one(cumsum(rep(ref,num_free_pulses))+15, parameter_vec, maxtime = seqlength)
ref.outcome.mat[mouse,ref.idx]=log(one.reference[seqlength])
}
one.random = sample(potential_actions,num_free_pulses, replace=T)
random.sds = c(random.sds, sd(one.random))
one.random = one.random+waitime_vec
ref.outcome.mat[mouse,length(refdays)+1]= log(generate_one(cumsum(one.random)+15, parameter_vec, maxtime = seqlength))[seqlength]
agent.action.mat[mouse,]=action.vec
agent.outcome.vec[mouse] = log(one.sequence[seqlength])
}
adj.val=1
plot(density(-agent.outcome.vec+ref.outcome.mat[,'random'],adjust=adj.val),col="purple",xlab="final ltv reference-final ltv agent", main="5 pulse performance")
colors= c("red","blue", "orange", "purple")
for(ref.idx in 1:(ncol(ref.outcome.mat))){
ref = refdays[ref.idx]
lines(density(ref.outcome.mat[,ref.idx] - agent.outcome.vec, adjust=adj.val), col=colors[ref.idx])
print(t.test(ref.outcome.mat[,ref.idx] - agent.outcome.vec))
delta = ref.outcome.mat[,ref.idx] - agent.outcome.vec
delta2 = ref.outcome.mat[,ref.idx] - ref.outcome.mat[,4]
effect.sizes[[references[ref.idx]]] = mean(delta)/sd(delta)
print(summary(delta))
print(length(delta[delta<=0])/test_num)
print(sum(delta2<0)/100)
}
legend("topright", legend=paste(c(paste("day", refdays),"random"), round(effect.sizes, digits=3), sep=": d="), pch=16, col=colors)
abline(v=0, lty=2)
